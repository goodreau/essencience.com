#!/bin/bash

# Ollama Setup & Test Script for Essencience.com
# This script starts Ollama containers and verifies the setup

set -e

echo "üöÄ Essencience.com - Ollama Local AI Pilot Setup"
echo "=================================================="
echo ""

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Check if Docker is running
echo -e "${BLUE}üì¶ Checking Docker...${NC}"
if ! docker info > /dev/null 2>&1; then
    echo -e "${RED}‚ùå Error: Docker is not running. Please start Docker first.${NC}"
    exit 1
fi
echo -e "${GREEN}‚úÖ Docker is running${NC}"
echo ""

# Navigate to project root
PROJECT_ROOT="/Volumes/EXTERNAL/Essencience.com"
if [ ! -d "$PROJECT_ROOT" ]; then
    echo -e "${RED}‚ùå Project root not found: $PROJECT_ROOT${NC}"
    exit 1
fi
cd "$PROJECT_ROOT"

# Start containers
echo -e "${BLUE}üê≥ Starting RAG containers...${NC}"
docker-compose -f docker-compose.rag.yml up -d

echo -e "${YELLOW}‚è≥ Waiting for services to initialize (15 seconds)...${NC}"
sleep 15

# Check Ollama
echo ""
echo -e "${BLUE}ü§ñ Checking Ollama...${NC}"
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Ollama is running on http://localhost:11434${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Ollama not responding yet, it may still be starting${NC}"
fi

# Check Qdrant
echo -e "${BLUE}üìä Checking Qdrant Vector Database...${NC}"
if curl -s http://localhost:6333/health > /dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Qdrant is running on http://localhost:6333${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Qdrant not responding yet${NC}"
fi

# Check Redis
echo -e "${BLUE}üìç Checking Redis...${NC}"
if redis-cli -h localhost -p 6379 ping > /dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Redis is running on localhost:6379${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Redis not responding yet${NC}"
fi

echo ""
echo -e "${BLUE}üì• Pulling LLM models...${NC}"
echo "This may take several minutes..."
echo ""

# Pull embedding model
echo -e "${YELLOW}‚è≥ Pulling nomic-embed-text (embeddings)...${NC}"
docker exec essencience-ollama ollama pull nomic-embed-text
echo -e "${GREEN}‚úÖ Embedding model downloaded${NC}"

# Pull main LLM
echo -e "${YELLOW}‚è≥ Pulling llama3.2 (main LLM)...${NC}"
docker exec essencience-ollama ollama pull llama3.2
echo -e "${GREEN}‚úÖ Main LLM model downloaded${NC}"

echo ""
echo -e "${BLUE}üìã Available models:${NC}"
docker exec essencience-ollama ollama list

echo ""
echo "=================================================="
echo -e "${GREEN}‚úÖ Setup Complete!${NC}"
echo "=================================================="
echo ""
echo -e "${BLUE}üîó Service URLs:${NC}"
echo "  ‚Ä¢ Ollama:    http://localhost:11434"
echo "  ‚Ä¢ Qdrant:    http://localhost:6333"
echo "  ‚Ä¢ Redis:     localhost:6379"
echo ""
echo -e "${BLUE}üìù Next Steps:${NC}"
echo "  1. Test Ollama: curl http://localhost:11434/api/tags"
echo "  2. Read OLLAMA_SETUP.md for detailed documentation"
echo "  3. Update laravel-temp/.env with OLLAMA settings"
echo "  4. Run: cd laravel-temp && php artisan serve"
echo "  5. Visit: http://localhost:8000 in your browser"
echo ""
echo -e "${BLUE}üíª Test the AI:${NC}"
echo "  1. Create a route: Route::get('/ai-chat', \\App\\Livewire\\AiChatbox::class)"
echo "  2. Visit: http://localhost:8000/ai-chat"
echo "  3. Start chatting with the AI!"
echo ""
echo -e "${YELLOW}‚ö†Ô∏è  Troubleshooting:${NC}"
echo "  ‚Ä¢ If Ollama doesn't respond: docker logs essencience-ollama"
echo "  ‚Ä¢ To stop containers: docker-compose -f docker-compose.rag.yml down"
echo "  ‚Ä¢ To remove all data: docker-compose -f docker-compose.rag.yml down -v"
echo ""
